= About Streaming Batch Aggregator

You can configure a batch aggregator scope to stream its content. +
This enables you to batch process all the records in the job instance, no matter how many or how large they are.

Instead of a list of elements that you receive with a fixed-size batch aggregator, the streaming functionality – an iterator – ensures that you receive all the records in a batch job without running out of memory.

For example, if you need to write millions of records, you can process the records as a streaming batch aggregator.

[source, xml, linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records >
		<batch:step name="batchStep">
			<batch:aggregator streaming="true">
				...
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
----

In general, you probably don't use batch streaming when sending data through an Anypoint Connector to a SaaS provider, like Salesforce, because SaaS providers often have restrictions on accepting streaming input. Rather, use streaming batch processing when writing to a file such as CSV, JSON, or XML.

When using a streaming aggregator, you can replace, change, or store the payload and variable data of each record.

By adding a foreach scope you can iterate trough the entire set of streaming records, and use the Groovy and the scripting module to modify the payload and create a variable for each collected record. +
You can sequentially go over each record's data and persistently store each record's payload and variables. This method of accessing records within the batch aggregator is called sequential access.

[source,xml,linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records>
		<batch:step name="batchStep">
			<batch:aggregator doc:name="batchAggregator" streaming="true">
				<foreach doc:name="For Each">
					<script:execute engine="groovy">
						<script:code>
              vars['marco'] = 'polo'
							vars['record'].payload = 'foo'
						</script:code>
					</script:execute>
				</foreach>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
----

The sequential access method assumes that:

. The aggregator size matches the amount of aggregated records.
. There is a direct correlation between the aggregated records and the items in the list.

Due to memory restrictions, random access is not supported for streaming aggregators. +
The record payloads for random access are exposed as an `immutable List` and since streaming aggregators implies having access to the entire set of records, without a fixed commit size the runtime can't guaranteed that all records will fit in memory.


== See Also

* link:/mule-user-guide/v/4.0/batch-aggregator-concept[About Batch Aggregator]
* link:/mule-user-guide/v/4.0/fix-batch-aggregator-concept[About Fixed Size Batch Aggregator]
