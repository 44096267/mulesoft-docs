= To Verify System Requirements

This topic provides information on how to verify that you system meets the minimum prerequisites to install and run the Anypoint Platform Private Cloud Edition.

== Disk Performance Verification

=== To Verify Disk Performance

To measure disk throughput, use a tool such as `hdparm`. On CentOS, `hdparm` can be installed by running

----
`sudo yum install -y hdparm`.
----

=== To Test Disk Throughput

All disks should have at least 300 MB/sec of throughput. Use the following command to verify the throughput of your disk:

----
sudo hdparm -d <device>
----

For example:

----
$ sudo hdparm -t /dev/sdd

/dev/sdd:
 Timing buffered disk reads: 4726 MB in  3.00 seconds = 1574.94 MB/sec
----

You can also measure throughput using the `dd`  tool. `dd` writes directly to the specified file, even if it is a device. You should not use this tool on a bare devices. Instead, after a device is formatted and mounted, you can write to a file on that device to measure throughput.

----
$ sudo mount /dev/sdb /var/lib/gravity # must be already formatted!
$ sudo dd if=/dev/zero of=/var/lib/gravity/testfile count=1000 bs=1M
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB, 1000 MiB) copied, 0.382535 s, 2.7 GB/s
----

`dd` provides less accurate information than `hdparm`, but it is available on any operating system and provides the ability to easily verify general disk performance.

=== Configure Disk IOPS

Depending on your hardware or virtualization provider, you may be able to configure disk IOPS (I/O Operations Per Second). Using a tool like `iops` (available here https://benjamin-schweizer.de/files/iops/iops-2011-02-11), you can verify available IOPS:

----
$ sudo ./iops --time 2 /dev/xvdb
/dev/xvdb, 107.37 GB, 32 threads:
 512   B blocks: 1893.0 IO/s, 946.5 KiB/s (  7.8 Mbit/s)
   1 KiB blocks: 1354.8 IO/s,   1.3 MiB/s ( 11.1 Mbit/s)
   2 KiB blocks: 1091.8 IO/s,   2.1 MiB/s ( 17.9 Mbit/s)
   4 KiB blocks:  807.1 IO/s,   3.2 MiB/s ( 26.4 Mbit/s)
   8 KiB blocks:  803.7 IO/s,   6.3 MiB/s ( 52.7 Mbit/s)
  16 KiB blocks:  787.4 IO/s,  12.3 MiB/s (103.2 Mbit/s)
  32 KiB blocks:  700.8 IO/s,  21.9 MiB/s (183.7 Mbit/s)
  64 KiB blocks:  590.0 IO/s,  36.9 MiB/s (309.3 Mbit/s)
 128 KiB blocks:  327.6 IO/s,  40.9 MiB/s (343.5 Mbit/s)
...
----


== Install Required Software Packages

*Yum* is an open-source command-line package-management utility for Linux operating systems using the RPM Package Manager.

Through Yum, install the tool LVM. LVM (orLogical Volume Manager) is a tool that adds a layer of abstraction between your operating system and the disks/partitions it uses. You can install LVM through the following command:

----
`sudo yum install lvm2`
----

[WARNING]
You must use a user with root access to perform this installation.

== Uninstall Unsupported Packages

The following sections describe software that may cause conflicts with Anypoint Platform Private Cloud Edition. You must uninstall these components before performing the installation.

=== Docker

Docker should be uninstalled from servers running the Anypoint Platform Private Cloud Edition. The Anypoint Platform installation includes a Docker package officially supported by Kubernetes.

=== Local DNS Servers

Local caching DNS servers listening on port 53 should be removed, e.g. named, dnsmasq, bind or others.


[[network-req]]
== Network Performance and Configuration Verification

=== Static IPs

All servers in the cluster should have static private IPv4 assigned to them, these must persist between server restarts. If IPs don’t persist between reboots, the cluster will enter a failed state.

=== VXLAN

This version of Kubernetes sets up overlay VXLAN and uses UDP transport to encapsulate traffic.

There’s direct communication between components of the cluster via TCP. The table below shows the ports used for inter-host communication:

[%header%autowidth.spread]
|===
|Protocol |Port/Range |Purpose
|TCP | 6060 | Health check
|TCP |7469 |Cluster control plane
|UDP |8472 |Overlay VXLAN network
|TCP |6443 |Kubernetes API server
|TCP |8080 |Kubernetes API server
|TCP |10248-10255 |Kubernetes Kubelet
|TCP |2379, 2380, 4001, 7001 |etcd distributed database
|TCP | 5000 | Docker registry
|TCP |3008-3010, 3023-3025, 3080, 7575|cluster control plane
|TCP |30000-32767 |Internal services port range
|TCP | 7000, 7011, 7199, 9042, 9160 | Cassandra
|TCP | 18080, 18443 | Object store cluster
|TCP | 5431-5435 | Database cluster
|TCP |61008-61010 | Installer port ranges (only used during install)
|TCP |61022-61024 | Installer port ranges (only used during install)
|===

=== NAT Traffic

Kubernetes overlay network uses NAT in some cases. This requires that servers should be able to send and receive packages with a source and destination that is different from server’s internal IP.

=== SSL Certificate

In order to use the Anypoint Platform, you must provide SSL credentials. You can upload a certificate through the Anypoint Platform UI, see link:/access-management/private-cloud-edition-features#security[Private Cloud Edition features]. This certificate must be trusted by every machine that’s connected to the platform.

[NOTE]
Keep in mind that you must register the same SSL certificate on every server with a Mule Runtime that is managed by this platform.

=== SMTP Server

Your network must include an SMTP server to manage e-mail alerts that are triggered by the platform. See link:/access-management/private-cloud-edition-features#smtp[on-prem features].

== Device Requirements

For the platform’s configuration you must assign two dedicated devices for use. One as a system state directory and the other as a target for Docker devicemapper configuration. These two directories must exist on every node of your cluster.

=== Anypoint system data device

The main purpose of the system state directory is storing system configuration and metadata - for example, database and packages among other things. As package sizes can be arbitrary large, it is important to estimate the minimum size requirements and allocate enough space as a dedicated device ahead of time.

This device will be formatted either as `xfs` or `ext4` and mounted as `/var/lib/gravity`. You can use the following shell snippet to guide this process (be sure to specify the correct device name in 2 places):

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/gravity
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/gravity\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-gravity.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-gravity.mount
sudo systemctl start var-lib-gravity.mount
----

=== etcd device

The main purpose of the etcd device is to provide dedicated storage for a distributed database used for cluster coordination. It does not require much space, 20GB should be enough.

This device will be formatted either as `xfs` or `ext4` and mounted as `/var/lib/gravity/planet/etcd`. You can use the following shell snippet to guide this process (be sure to specify the correct device name in 2 places):

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/gravity/planet/etcd
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/gravity/planet/etcd\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-gravity-planet-etcd.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-gravity-planet-etcd.mount
sudo systemctl start var-lib-gravity-planet-etcd.mount
----


=== Anypoint application data device

The main purpose of application data directory is storing application configuration and data. The amount of space required should be at minimum 250GB, but might vary depending on your specific use case. It is important to estimate the minimum size requirements and allocate enough space as a dedicated device ahead of time.

This device will be formatted either as `xfs` or `ext4` and mounted as `/var/lib/data`. You can use the following shell snippet to guide this process (be sure to specify the correct device name in 2 places):

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/data
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/data\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-data.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-data.mount
sudo systemctl start var-lib-data.mount
----

=== Docker device

This device is used by Docker’s Device Mapper storage driver.

Unless specified, Docker configuration defaults to the use of Device Mapper in loopback mode (using /dev/loopX devices) which is not recommended for production. To configure Docker to use a dedicated device for Device Mapper storage driver, an unformatted device (or a partition) (i.e. /dev/sdd) can be provided during installation. This directory will be automatically configured and set up for use.

Unformatted devices potentially usable for system directory / Device Mapper are automatically discovered by agents running on each node. Discovered devices are offered on a drop-down menu for configuration before the installation is started.

[TIP]
====
You can list unmounted devices with the following command:
---
lsblk --output=NAME,TYPE,SIZE,FSTYPE -P -I 8,9,202|grep 'FSTYPE=""'
---
====

The unmounted devices have an empty value in FSTYPE column. Devices with TYPE="part" are partitions on another device.
Note, that we only list specific device types:
|===
|Device type|Description
|8   |SCSI disk devices
|9   |Metadisk (RAID) devices
|202 |Xen virtual block devices (Amazon EC2)
|===

==== Manually resetting devices/partitions
Logical Volume Manager allows one to group multiple physical volumes into a single storage volume (Volume Group) and then divide these into Logical
Volumes. Physical Volumes are either a whole device or a partition.

In rare cases when a device is in use by another Logical Volume or you want to manually reset a device previously configured for Device Mapper, here is
a list of commands that will help:

Logical Volume Manager toolset consists of the following commands:
  * dmsetup - is a low-level logical volume management
  * pv/vg/lv-prefixed commands like pvdisplay and pvcreate/pvremove - for working with specific LVM object types (i.e. lv - for logical volumes and vg - for volume groups)

In order to reset a device, you will have to work backwards:
  * remove logical volume with `lvremove -f docker/thinpool` (use `lvdisplay` to find the volume to remove)
  * remove volume group with `vgremovei docker` (use `vgdisplay` to locate the volume group to remove)
  * remove physical volume and reset device with `pvremove /dev/<device name>` (use `pvdisplay` to find the physical volume to remove and the device name it is on)

==== See Also
 link:https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/[Docker and the DeviceMapper Storage Driver]
 link:https://linuxconfig.org/linux-lvm-logical-volume-manager[Linux lvm - Logical Volume Manager]
 link:https://www.kernel.org/doc/Documentation/devices.txt[Linux allocated devices]

[NOTE]
It is strongly recommended to have at least 100Gb sized device for the Device Mapper directory - with devices 50Gb and less the system performance will degrade dramatically or might not work at all.
