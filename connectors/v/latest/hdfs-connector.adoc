= Hadoop HDFS Connector
:keywords: anypoint studio, connectors, hdfs
:imagesdir: ./_images

The Anypoint Connector for the Hadoop Distributed File System (HDFS) is used as a bi-directional gateway between Mule applications and HDFS.

////
link:/release-notes/hdfs-connector-release-notes[Connector Release Notes] +
////

http://mulesoft.github.io/hdfs-connector/[Technical Reference]

https://www.anypoint.mulesoft.com/exchange/org.mule.modules/mule-module-hdfs/[Anypoint Exchange]

Since version 5.0.0, MuleSoft maintains this connector under the https://www.mulesoft.com/legal/versioning-back-support-policy#anypoint-connectors[_Select_] support policy.

== Prerequisites

This document assumes that you are familiar with Mule,
Anypoint Connectors, and Anypoint Studio Essentials. This page requires some basic knowledge of Mule Concepts, Elements in a Mule Flow, and Global Elements.

To use the HDFS connector, you need:

* Anypoint Studio - If you do not use Anypoint Studio for development, follow the instructions in <<Configuring Maven Dependencies,Configuring Maven Dependencies>> for your project.
* An instance of Hadoop Distributed File System  up and running, which can be downloaded from http://hadoop.apache.org/releases.html[here].

== Compatibility

HDFS Hadoop connector is compatible with the following:

[%header,width="100%",cols="50%,50%"]
|===
|Application/Service|Version
|Mule Runtime |3.6 or newer
|Apache Hadoop |2.7.1 or newer
|===

Starting with v5.0.0, the HDFS connector is licensed commercially with Anypoint Platform as are other https://www.mulesoft.com/legal/versioning-back-support-policy#anypoint-connectors[_Select_] connectors.  Prior versions will remain freely available to the community.

== To Install this Connector

. In Anypoint Studio, click the Exchange icon in the Studio taskbar.
. Click Login in Anypoint Exchange.
. Search for this connector and click Install.
. Follow the prompts to install the connector.

=== To Update from an Older Version

When an updated version of a connector is released, Anypoint Studio displays the Updates Available popup in the bottom right corner of Studio. Click the popup and install the new version.

== Configure the Connector Global Element

You can configure a global HDFS element that can be used by the connector. The HDFS connector offers the following global configuration options, requiring credentials.

=== Simple Authentication Configuration

[%header,cols="50a,50a"]
|===
|Field |Description
|NameNode URI |The URI of the file system to connect to.

This is passed to the HDFS client as the FileSystem#FS_DEFAULT_NAME_KEY configuration entry. It can be overridden by values in configurationResources and configurationEntries.
|Username | User identity that Hadoop uses for permissions in HDFS.

When Simple Authentication is used, Hadoop requires the user to be set as a System Property called HADOOP_USER_NAME. If you fill this field then the connector sets it for you, however you can set it by yourself. If the variable is not set, Hadoop  uses the current logged in OS user.
|Configuration Resources |A list of configuration resource files to be loaded by the HDFS client. Here you can provide additional configuration files. (for examplem core-site.xml)
|Configuration Entries |A map of configuration entries to be used by the HDFS client. Here you can provide additional configuration entries as key/value pairs.
|===

image:hdfs-config.png[hdfs-config]


=== Kerberos Authentication Configuration

[%header,width="100a",cols="50a,50a"]
|===
|Field |Description
|NameNode URI |The URI of the file system to connect to.

This is passed to HDFS client as the FileSystem#FS_DEFAULT_NAME_KEY configuration entry. It can be overridden by values in configurationResources and configurationEntries.
|Username | Kerberos principal.

This is passed to HDFS client as the "hadoop.job.ugi" configuration entry. It can be overridden by values in configurationResources and configurationEntries. If not provided it will use the currently logged in user.
|KeytabPath |Path to the link:https://web.mit.edu/kerberos/krb5-1.12/doc/basic/keytab_def.html[keytab file] associated with username.

KeytabPath is used to obtain a TGT from the authorization server.  If not provided the connector looks for a TGT associated to username within your local kerberos cache.
|Configuration Resources |A list of configuration resource files to be loaded by the HDFS client. Here you can provide additional configuration files. (for example, core-site.xml)
|Configuration Entries |A map of configuration entries to be used by the HDFS client. Here you can provide additional configuration entries as key/value pairs.
|===

image:hdfs-config-with-kerberos.png[hdfs-config-with-kerberos]

== To Use the Connector

You can use this connector as an inbound endpoint for polling content of a file at a configurable rate (interval) or as an outbound connector for manipulating data into the HDFS server.

See the link:/mulesoft.github.io/hdfs-connector[full list of operations for the connector].

=== About the Connector Namespace and Schema

When designing your application in Studio, the act of dragging the connector from the palette onto the Anypoint Studio canvas should automatically populate the XML code with the connector namespace and schema location.

*Namespace:* `http://www.mulesoft.org/schema/mule/hdfs` +
*Schema Location:* `http://www.mulesoft.org/schema/mule/connector/current/mule-hdfs.xsd`

[TIP]
If you are manually coding the Mule application in Studio's XML editor or other text editor, define the namespace and schema location in the header of your Configuration XML, inside the `<mule>` tag.

[source, xml,linenums]
----
<mule xmlns="http://www.mulesoft.org/schema/mule/core"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xmlns:connector="http://www.mulesoft.org/schema/mule/hdfs"
      xsi:schemaLocation="
               http://www.mulesoft.org/schema/mule/core
               http://www.mulesoft.org/schema/mule/core/current/mule.xsd
               http://www.mulesoft.org/schema/mule/connector
               http://www.mulesoft.org/schema/mule/connector/current/mule-hdfs.xsd">

      <!-- put your global configuration elements and flows here -->

</mule>
----

=== To Use the Connector in a Mavenized Mule App

If you are coding a Mavenized Mule application, this XML snippet must be included in your `pom.xml` file.

[source,xml,linenums]
----
<dependency>
  <groupId>org.mule.modules</groupId>
      <artifactId>mule-module-hdfs</artifactId>
      <version>5.0.0</version>
</dependency>
----

In the `<version>` tags, put the desired version number, the word `RELEASE` for the latest release, or `SNAPSHOT` for the latest available version.

== About the Demo Mule Application for this Connector

Existing demos demonstrate how to use the connector for http://mulesoft.github.io/hdfs-connector/[basic file system operations] and how to http://mulesoft.github.io/hdfs-connector/[poll data from a file] at a specific interval.

=== Example: Use Case

The following example shows how to create a text file into HDFS using the connector:

. In Anypoint Studio, click File > New > Mule Project, name the project, and click OK.
. In the search field, type "http" and drag the HTTP connector to the canvas, click the green plus sign to the right of Connector Configuration, and in the next screen, click OK to accept the default settings. Name the endpoint /createFile.
. In the Search bar type "HDFS" and drag the HDFS connector onto the canvas. Configure as explained <<Configure the Connector Global Element>>
. Choose Write to path as an operation. Set Path to `/test.txt` (this is the path of the file that is going to be created into HDFS) and leave other options with default values.
. The flow should look like this:
+
image:hdfs-create-file-flow.png[Create file flow]
+
. Run the application. From your favorite HTTP client make a POST request with "Content-type:plain/text" to `locahost:8081/createFile` with content that you want to write as payload. (e.g. `curl -X POST -H "Content-Type:plain/text" -d "payload to write to file" localhost:8090/createFile`)
. Check that /test.txt has been created and has your content by using Hadoop explorer.


=== To Create a File into HDFS - XML

Paste this into Anypoint Studio to interact with the example use case application discussed in this guide.

[source,xml,linenums]
----
<?xml version="1.0" encoding="UTF-8"?>

<mule xmlns:hdfs="http://www.mulesoft.org/schema/mule/hdfs" xmlns:http="http://www.mulesoft.org/schema/mule/http" xmlns="http://www.mulesoft.org/schema/mule/core" xmlns:doc="http://www.mulesoft.org/schema/mule/documentation"
	xmlns:spring="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-current.xsd
http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
http://www.mulesoft.org/schema/mule/hdfs http://www.mulesoft.org/schema/mule/hdfs/current/mule-hdfs.xsd">
    <http:listener-config name="HTTP_Listener_Configuration" host="0.0.0.0" port="8081" doc:name="HTTP Listener Configuration"/>
    <hdfs:config name="HDFS__Configuration" nameNodeUri="hdfs://localhost:9000" doc:name="HDFS: Configuration"/>
    <flow name="hdfs-example-use-caseFlow">
        <http:listener config-ref="HTTP_Listener_Configuration" path="/createFile" doc:name="HTTP"/>
        <hdfs:write config-ref="HDFS__Configuration" path="/test.txt" doc:name="HDFS"/>
    </flow>
</mule>
----

== To Improve Connector Performance

To define the pooling profile for the connector manually, access the Pooling Profile tab in the applicable global element for the connector.

////
For background information on pooling, see link:/mule-user-guide/v/3.8/tuning-performance[Tuning Performance].
////
